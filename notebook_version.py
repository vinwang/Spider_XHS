"""
å°çº¢ä¹¦ç¬”è®°çˆ¬è™« - é­”å¡”ç¤¾åŒºNotebookç‰ˆæœ¬
=====================================

åŠŸèƒ½ï¼š
- æœç´¢å¹¶çˆ¬å–å°çº¢ä¹¦é«˜äº’åŠ¨ç¬”è®°
- æ”¯æŒæŒ‰ç‚¹èµæ•°ã€æ”¶è—æ•°ç­›é€‰
- è‡ªåŠ¨ä¿å­˜ä¸ºExcelæ ¼å¼

ä½¿ç”¨å‰å‡†å¤‡ï¼š
1. ç¡®ä¿å·²é…ç½®Cookieï¼ˆè§ä¸‹æ–¹é…ç½®ï¼‰
2. å®‰è£…ä¾èµ–ï¼špip install -r requirements.txt
3. è¿è¡Œä»£ç å—

æ³¨æ„ï¼š
- Cookieå¯èƒ½ä¼šè¿‡æœŸï¼Œéœ€è¦å®šæœŸæ›´æ–°
- è¯·å‹¿è¿‡äºé¢‘ç¹è¯·æ±‚ï¼Œéµå®ˆå¹³å°è§„åˆ™
- ä»…ä¾›å­¦ä¹ ç ”ç©¶ä½¿ç”¨
"""

# @title ğŸ”§ é…ç½®åŒºåŸŸ - è¯·å¡«å†™ä½ çš„Cookie
# @markdown > è·å–Cookieæ–¹æ³•ï¼šç™»å½•å°çº¢ä¹¦åï¼ŒæŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œåœ¨Networkæ ‡ç­¾é¡µæ‰¾åˆ°è¯·æ±‚çš„Cookie

COOKIE = ""  # @param {type:"string"}

# @title ğŸ“Š çˆ¬å–å‚æ•°è®¾ç½®
# @markdown è®¾ç½®æœç´¢å…³é”®è¯å’Œç­›é€‰æ¡ä»¶

QUERY = "aiæç¤ºè¯"  # @param {type:"string"}
QUERY_NUM = 30  # @param {type:"integer", min:1, max:200}
MIN_LIKES = 1000  # @param {type:"integer", min:0}
MIN_COLLECTS = 2000  # @param {type:"integer", min:0}
SORT_TYPE = "æœ€å¤šç‚¹èµ"  # @param ["ç»¼åˆæ’åº", "æœ€æ–°", "æœ€å¤šç‚¹èµ", "æœ€å¤šè¯„è®º", "æœ€å¤šæ”¶è—"]
NOTE_TYPE = "ä¸é™"  # @param ["ä¸é™", "è§†é¢‘ç¬”è®°", "æ™®é€šç¬”è®°"]
SAVE_CHOICE = "åªä¿å­˜Excel"  # @param ["åªä¿å­˜Excel", "åªä¿å­˜åª’ä½“æ–‡ä»¶", "ä¿å­˜æ‰€æœ‰ï¼ˆExcel+åª’ä½“ï¼‰"]

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…"""
    import subprocess
    import sys
    
    packages = [
        'requests',
        'loguru', 
        'python-dotenv',
        'retry',
        'openpyxl',
        'PyExecJS'
    ]
    
    print("ğŸ“¦ æ­£åœ¨å®‰è£…ä¾èµ–åŒ…...")
    for package in packages:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])
            print(f"  âœ… {package}")
        except Exception as e:
            print(f"  âš ï¸ {package} å®‰è£…å¤±è´¥: {e}")
    
    print("âœ¨ ä¾èµ–å®‰è£…å®Œæˆï¼\n")


def init():
    """åˆå§‹åŒ–çˆ¬è™«ç¯å¢ƒ"""
    import os
    
    # è®¾ç½®ä¿å­˜è·¯å¾„
    base_path = {
        'excel': os.path.abspath('datas/excel_datas'),
        'media': os.path.abspath('datas/media_datas')
    }
    
    # åˆ›å»ºç›®å½•
    for path in base_path.values():
        os.makedirs(path, exist_ok=True)
    
    return COOKIE, base_path


def parse_number(num_str):
    """
    å°†å­—ç¬¦ä¸²æ ¼å¼çš„æ•°å­—è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¦‚ '2.7ä¸‡' -> 27000
    """
    if isinstance(num_str, int):
        return num_str
    elif isinstance(num_str, str):
        num_str = num_str.strip()
        if 'ä¸‡' in num_str:
            try:
                return int(float(num_str.replace('ä¸‡', '')) * 10000)
            except ValueError:
                return 0
        elif 'åƒ' in num_str:
            try:
                return int(float(num_str.replace('åƒ', '')) * 1000)
            except ValueError:
                return 0
        else:
            try:
                return int(num_str)
            except ValueError:
                return 0
    else:
        return 0


def handle_note_info(data):
    """å¤„ç†ç¬”è®°ä¿¡æ¯"""
    import time
    
    def timestamp_to_str(timestamp):
        time_local = time.localtime(timestamp / 1000)
        return time.strftime("%Y-%m-%d %H:%M:%S", time_local)
    
    try:
        note_type = data['note_card']['type']
        if note_type == 'normal':
            note_type = 'å›¾é›†'
        else:
            note_type = 'è§†é¢‘'
        
        # è§£æäº’åŠ¨æ•°æ®
        liked_count = parse_number(data['note_card']['interact_info']['liked_count'])
        collected_count = parse_number(data['note_card']['interact_info']['collected_count'])
        comment_count = parse_number(data['note_card']['interact_info']['comment_count'])
        share_count = parse_number(data['note_card']['interact_info']['share_count'])
        
        # å¤„ç†å›¾ç‰‡åˆ—è¡¨
        image_list_temp = data['note_card']['image_list']
        image_list = []
        for image in image_list_temp:
            try:
                image_list.append(image['info_list'][1]['url'])
            except:
                pass
        
        # å¤„ç†è§†é¢‘ä¿¡æ¯
        video_cover = None
        video_addr = None
        if note_type == 'è§†é¢‘':
            try:
                if image_list:
                    video_cover = image_list[0]
                if 'video' in data['note_card'] and 'consumer' in data['note_card']['video']:
                    video_addr = 'https://sns-video-bd.xhscdn.com/' + data['note_card']['video']['consumer']['origin_video_key']
            except:
                pass
        
        # å¤„ç†æ ‡ç­¾
        tags_temp = data['note_card']['tag_list']
        tags = [tag['name'] for tag in tags_temp if 'name' in tag]
        
        # IPå½’å±åœ°
        ip_location = data['note_card'].get('ip_location', 'æœªçŸ¥')
        
        return {
            'note_id': data['id'],
            'note_url': data['url'],
            'note_type': note_type,
            'user_id': data['note_card']['user']['user_id'],
            'home_url': f"https://www.xiaohongshu.com/user/profile/{data['note_card']['user']['user_id']}",
            'nickname': data['note_card']['user']['nickname'],
            'avatar': data['note_card']['user']['avatar'],
            'title': data['note_card']['title'].strip() or 'æ— æ ‡é¢˜',
            'desc': data['note_card']['desc'],
            'liked_count': liked_count,
            'collected_count': collected_count,
            'comment_count': comment_count,
            'share_count': share_count,
            'video_cover': video_cover,
            'video_addr': video_addr,
            'image_list': image_list,
            'tags': tags,
            'upload_time': timestamp_to_str(data['note_card']['time']),
            'ip_location': ip_location,
        }
    except Exception as e:
        print(f"å¤„ç†ç¬”è®°ä¿¡æ¯å¤±è´¥: {e}")
        return None


def save_to_xlsx(datas, file_path, search_query=''):
    """ä¿å­˜æ•°æ®åˆ°Excel"""
    import openpyxl
    
    wb = openpyxl.Workbook()
    ws = wb.active
    
    # è®¾ç½®è¡¨å¤´
    headers = [
        'æœç´¢è¯', 'æ ‡é¢˜', 'æè¿°', 'æ ‡ç­¾', 'ç‚¹èµæ•°é‡', 'æ”¶è—æ•°é‡', 
        'è¯„è®ºæ•°é‡', 'åˆ†äº«æ•°é‡', 'ç¬”è®°url', 'ç¬”è®°id', 'ç”¨æˆ·id', 
        'ç”¨æˆ·ä¸»é¡µurl', 'æ˜µç§°', 'å¤´åƒurl', 'å›¾ç‰‡åœ°å€urlåˆ—è¡¨', 
        'è§†é¢‘å°é¢url', 'è§†é¢‘åœ°å€url', 'ä¸Šä¼ æ—¶é—´', 'ipå½’å±åœ°'
    ]
    ws.append(headers)
    
    # å†™å…¥æ•°æ®
    for data in datas:
        row_data = [
            search_query,
            data.get('title', ''),
            data.get('desc', ''),
            str(data.get('tags', [])),
            data.get('liked_count', 0),
            data.get('collected_count', 0),
            data.get('comment_count', 0),
            data.get('share_count', 0),
            data.get('note_url', ''),
            data.get('note_id', ''),
            data.get('user_id', ''),
            data.get('home_url', ''),
            data.get('nickname', ''),
            data.get('avatar', ''),
            str(data.get('image_list', [])),
            data.get('video_cover', ''),
            data.get('video_addr', ''),
            data.get('upload_time', ''),
            data.get('ip_location', ''),
        ]
        ws.append(row_data)
    
    wb.save(file_path)
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")


def get_note_info(note_url, cookies_str):
    """è·å–å•æ¡ç¬”è®°è¯¦æƒ…"""
    import urllib.parse
    import requests
    import json
    import time
    
    def generate_x_b3_traceid(length=16):
        import random
        import string
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))
    
    def get_common_headers():
        return {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.xiaohongshu.com/',
        }
    
    def splice_str(api, params):
        if params:
            query_string = '&'.join([f"{key}={value}" for key, value in params.items()])
            return f"{api}?{query_string}"
        return api
    
    def generate_request_params(cookies_str, api, data, method='GET'):
        import execjs
        import random
        
        headers = get_common_headers()
        headers['Accept'] = 'application/json, text/plain, */*'
        headers['Content-Type'] = 'application/json'
        
        # è¯»å–xsæ–‡ä»¶
        try:
            with open('static/xs-common-1128.js', 'r', encoding='utf-8') as f:
                xs_code = f.read()
            
            # è¯»å–x-sæ–‡ä»¶
            with open('static/xhs_xray.js', 'r', encoding='utf-8') as f:
                x_s_code = f.read()
            
            # ç¼–è¯‘JSä»£ç 
            xs_compiled = execjs.compile(xs_code)
            x_s_compiled = execjs.compile(x_s_code)
            
            # è·å–x_så€¼
            x_s = x_s_compiled.call('get_x_s', api, json.dumps(data) if data else '', method)
            
            # è·å–xså€¼
            ctx = execjs.compile(xs_code + '\n' + x_s_code)
            xs_value = ctx.call('getXs', api, '2.0', x_s, '0')
            
            cookies = {}
            if cookies_str:
                for item in cookies_str.split(';'):
                    if '=' in item:
                        key, value = item.strip().split('=', 1)
                        cookies[key.strip()] = value.strip()
            
            # æ„å»ºheaders
            headers['x_s'] = x_s
            headers['x_t'] = str(int(time.time() * 1000))
            headers['x_trace_id'] = generate_x_b3_traceid()
            headers['xs'] = xs_value
            
            return headers, cookies, json.dumps(data) if data else ''
            
        except Exception as e:
            print(f"ç”Ÿæˆè¯·æ±‚å‚æ•°å¤±è´¥: {e}")
            return get_common_headers(), {}, ''
    
    try:
        urlParse = urllib.parse.urlparse(note_url)
        note_id = urlParse.path.split("/")[-1]
        kvs = urlParse.query.split('&')
        kvDist = {kv.split('=')[0]: kv.split('=')[1] for kv in kvs}
        
        api = "/api/sns/web/v1/feed"
        data = {
            "source_note_id": note_id,
            "image_formats": ["jpg", "webp", "avif"],
            "extra": {"need_body_topic": "1"},
            "xsec_source": kvDist.get('xsec_source', "pc_search"),
            "xsec_token": kvDist.get('xsec_token', '')
        }
        
        headers, cookies, post_data = generate_request_params(cookies_str, api, data, 'POST')
        
        response = requests.post(
            'https://edith.xiaohongshu.com' + api,
            headers=headers,
            data=post_data,
            cookies=cookies,
            timeout=10
        )
        
        result = response.json()
        
        if result.get('success') and result.get('data', {}).get('items'):
            note_data = result['data']['items'][0]
            note_data['url'] = note_url
            return True, "æˆåŠŸ", note_data
        else:
            return False, f"è·å–å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}", None
            
    except Exception as e:
        return False, str(e), None


def search_notes(query, require_num, cookies_str, sort_type_choice, note_type):
    """æœç´¢ç¬”è®°"""
    import urllib.parse
    import requests
    import json
    import time
    
    def generate_x_b3_traceid(length=16):
        import random
        import string
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))
    
    def get_common_headers():
        return {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.xiaohongshu.com/',
        }
    
    def splice_str(api, params):
        if params:
            query_string = '&'.join([f"{key}={value}" for key, value in params.items()])
            return f"{api}?{query_string}"
        return api
    
    def generate_request_params(cookies_str, api, data, method='GET'):
        import execjs
        import random
        
        headers = get_common_headers()
        headers['Accept'] = 'application/json, text/plain, */*'
        headers['Content-Type'] = 'application/json'
        
        try:
            with open('static/xs-common-1128.js', 'r', encoding='utf-8') as f:
                xs_code = f.read()
            
            with open('static/xhs_xray.js', 'r', encoding='utf-8') as f:
                x_s_code = f.read()
            
            xs_compiled = execjs.compile(xs_code)
            x_s_compiled = execjs.compile(x_s_code)
            
            x_s = x_s_compiled.call('get_x_s', api, json.dumps(data) if data else '', method)
            
            ctx = execjs.compile(xs_code + '\n' + x_s_code)
            xs_value = ctx.call('getXs', api, '2.0', x_s, '0')
            
            cookies = {}
            if cookies_str:
                for item in cookies_str.split(';'):
                    if '=' in item:
                        key, value = item.strip().split('=', 1)
                        cookies[key.strip()] = value.strip()
            
            headers['x_s'] = x_s
            headers['x_t'] = str(int(time.time() * 1000))
            headers['x_trace_id'] = generate_x_b3_traceid()
            headers['xs'] = xs_value
            
            return headers, cookies, json.dumps(data) if data else ''
            
        except Exception as e:
            print(f"ç”Ÿæˆè¯·æ±‚å‚æ•°å¤±è´¥: {e}")
            return get_common_headers(), {}, ''
    
    # è½¬æ¢æ’åºå‚æ•°
    sort_map = {
        "ç»¼åˆæ’åº": 0,
        "æœ€æ–°": 1,
        "æœ€å¤šç‚¹èµ": 2,
        "æœ€å¤šè¯„è®º": 3,
        "æœ€å¤šæ”¶è—": 4
    }
    
    note_type_map = {
        "ä¸é™": 0,
        "è§†é¢‘ç¬”è®°": 1,
        "æ™®é€šç¬”è®°": 2
    }
    
    sort_type = "general"
    if sort_type_choice == 1:
        sort_type = "time_descending"
    elif sort_type_choice == 2:
        sort_type = "popularity_descending"
    elif sort_type_choice == 3:
        sort_type = "comment_descending"
    elif sort_type_choice == 4:
        sort_type = "collect_descending"
    
    filter_note_type = "ä¸é™"
    if note_type == 1:
        filter_note_type = "è§†é¢‘ç¬”è®°"
    elif note_type == 2:
        filter_note_type = "æ™®é€šç¬”è®°"
    
    page = 1
    note_list = []
    
    try:
        while len(note_list) < require_num:
            api = "/api/sns/web/v1/search/notes"
            data = {
                "keyword": query,
                "page": page,
                "page_size": 20,
                "search_id": generate_x_b3_traceid(21),
                "sort": sort_type,
                "note_type": 0,
                "ext_flags": [],
                "filters": [
                    {"tags": [sort_type], "type": "sort_type"},
                    {"tags": [filter_note_type], "type": "filter_note_type"},
                    {"tags": ["ä¸é™"], "type": "filter_note_time"},
                    {"tags": ["ä¸é™"], "type": "filter_note_range"},
                    {"tags": ["ä¸é™"], "type": "filter_pos_distance"}
                ],
                "geo": "",
                "image_formats": ["jpg", "webp", "avif"]
            }
            
            headers, cookies, post_data = generate_request_params(cookies_str, api, data, 'POST')
            
            response = requests.post(
                'https://edith.xiaohongshu.com' + api,
                headers=headers,
                data=post_data.encode('utf-8'),
                cookies=cookies,
                timeout=10
            )
            
            result = response.json()
            
            if result.get('success') and result.get('data', {}).get('items'):
                notes = result['data']['items']
                note_list.extend(notes)
                page += 1
                
                if not result['data'].get('has_more'):
                    break
            else:
                print(f"æœç´¢å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                break
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        return True, "æœç´¢æˆåŠŸ", note_list[:require_num]
        
    except Exception as e:
        return False, str(e), None


def crawl_xiaohongshu():
    """ä¸»çˆ¬å–å‡½æ•°"""
    import os
    from datetime import datetime
    
    print("=" * 60)
    print("ğŸš€ å°çº¢ä¹¦ç¬”è®°çˆ¬è™« - é­”å¡”ç¤¾åŒºNotebookç‰ˆæœ¬")
    print("=" * 60)
    print(f"ğŸ“… è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ” æœç´¢å…³é”®è¯: {QUERY}")
    print(f"ğŸ“Š æœç´¢æ•°é‡: {QUERY_NUM}")
    print(f"ğŸ‘ æœ€ä½ç‚¹èµ: {MIN_LIKES}")
    print(f"â­ æœ€ä½æ”¶è—: {MIN_COLLECTS}")
    print("-" * 60)
    
    # éªŒè¯Cookie
    if not COOKIE:
        print("âŒ é”™è¯¯: è¯·å…ˆé…ç½®Cookieï¼")
        print("ğŸ“ è·å–æ–¹æ³•:")
        print("  1. ç™»å½•å°çº¢ä¹¦ç½‘ç«™")
        print("  2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("  3. åˆ‡æ¢åˆ°Networkæ ‡ç­¾")
        print("  4. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°è¯·æ±‚çš„Cookie")
        print("  5. å¤åˆ¶å®Œæ•´çš„Cookieå­—ç¬¦ä¸²")
        return
    
    # åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ–ä¸­...")
    cookies_str, base_path = init()
    print("âœ… åˆå§‹åŒ–å®Œæˆï¼\n")
    
    # æœç´¢ç¬”è®°
    print(f"ğŸ” å¼€å§‹æœç´¢ '{QUERY}' ...")
    
    # è½¬æ¢å‚æ•°
    sort_map = {"ç»¼åˆæ’åº": 0, "æœ€æ–°": 1, "æœ€å¤šç‚¹èµ": 2, "æœ€å¤šè¯„è®º": 3, "æœ€å¤šæ”¶è—": 4}
    note_type_map = {"ä¸é™": 0, "è§†é¢‘ç¬”è®°": 1, "æ™®é€šç¬”è®°": 2}
    
    success, msg, notes = search_notes(
        QUERY, QUERY_NUM, COOKIE,
        sort_map.get(SORT_TYPE, 2),
        note_type_map.get(NOTE_TYPE, 0)
    )
    
    if not success or not notes:
        print(f"âŒ æœç´¢å¤±è´¥: {msg}")
        return
    
    print(f"âœ… æœç´¢åˆ° {len(notes)} æ¡ç¬”è®°\n")
    
    # è¿‡æ»¤ç¬”è®°
    notes = [n for n in notes if n.get('model_type') == 'note']
    print(f"ğŸ“‹ æœ‰æ•ˆç¬”è®°æ•°é‡: {len(notes)}\n")
    
    # çˆ¬å–ç¬”è®°è¯¦æƒ…
    print("ğŸ“¥ å¼€å§‹çˆ¬å–ç¬”è®°è¯¦æƒ…...")
    filtered_notes = []
    total = len(notes)
    
    for i, note in enumerate(notes, 1):
        print(f"  è¿›åº¦: {i}/{total} ({int(i/total*100)}%)", end='\r')
        
        note_url = f"https://www.xiaohongshu.com/explore/{note['id']}?xsec_token={note['xsec_token']}"
        success, msg, note_info = get_note_info(note_url, COOKIE)
        
        if success and note_info:
            note_info = handle_note_info(note_info)
            if note_info:
                # ç­›é€‰é«˜äº’åŠ¨ç¬”è®°
                if (note_info['liked_count'] > MIN_LIKES or 
                    note_info['collected_count'] > MIN_COLLECTS):
                    filtered_notes.append(note_info)
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)
    
    print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“Š åŸå§‹ç¬”è®°æ•°é‡: {total}")
    print(f"âœ¨ ç¬¦åˆæ¡ä»¶æ•°é‡: {len(filtered_notes)}\n")
    
    if not filtered_notes:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç¬”è®°")
        print("ğŸ’¡ å»ºè®®: é™ä½ç­›é€‰æ¡ä»¶ï¼ˆç‚¹èµæ•°æˆ–æ”¶è—æ•°ï¼‰")
        return
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜æ•°æ®ä¸­...")
    
    # è½¬æ¢ä¿å­˜é€‰é¡¹
    save_map = {
        "åªä¿å­˜Excel": "excel",
        "åªä¿å­˜åª’ä½“æ–‡ä»¶": "media", 
        "ä¿å­˜æ‰€æœ‰ï¼ˆExcel+åª’ä½“ï¼‰": "all"
    }
    
    save_choice = save_map.get(SAVE_CHOICE, 'excel')
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"{QUERY}_{len(filtered_notes)}æ¡ç¬”è®°"
    excel_path = os.path.join(base_path['excel'], f"{filename}.xlsx")
    
    # ä¿å­˜Excel
    save_to_xlsx(filtered_notes, excel_path, QUERY)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ Excelæ–‡ä»¶: {excel_path}")
    print(f"ğŸ“Š ç¬¦åˆæ¡ä»¶ç¬”è®°: {len(filtered_notes)} æ¡")
    print("-" * 60)
    
    # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®é¢„è§ˆ
    print("\nğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰3æ¡ï¼‰:")
    for i, note in enumerate(filtered_notes[:3], 1):
        print(f"  {i}. {note['title'][:30]}...")
        print(f"     ğŸ‘ {note['liked_count']}  â­ {note['collected_count']}")
    
    print("\nğŸ’¡ æç¤º:")
    print("  - Excelæ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥ä¸‹è½½åˆ°æœ¬åœ°")
    print("  - Cookieå¯èƒ½ä¼šè¿‡æœŸï¼Œéœ€è¦å®šæœŸæ›´æ–°")
    print("  - è¯·å‹¿è¿‡äºé¢‘ç¹è¯·æ±‚")
    print("=" * 60)


if __name__ == "__main__":
    # å®‰è£…ä¾èµ–
    install_dependencies()
    
    # è¿è¡Œçˆ¬è™«
    crawl_xiaohongshu()
